* 机器学习
  KNN 算法中的问题：
  1) K 的取值
  2) K != n|c|, c 时候种类数
  3) 权重的选择，高斯衰减
  4) voronoi 空间结构

* 高斯衰减
  + [[https://www.cnblogs.com/bigmonkey/p/7387943.html][k最邻近算法——加权kNN - 我是8位的 - 博客园]]

* 算法步骤
  1. 选择算法，算法的参数不同可以得到不同的函数
  2. 确定 Loss 函数，即：确定优化方向
  3. 训练函数，得到最佳参数

* ravel & T
  #+BEGIN_SRC python
    >>> arr = np.full((10, 1), 1)
    >>> arr
    array([[1],
           [1],
           [1],
           [1],
           [1],
           [1],
           [1],
           [1],
           [1],
           [1]])
    >>> arr.T
    array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
    >>> arr.ravel()
    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
  #+END_SRC

* 向量
  列或行数目为 1
* KNN
  + 高斯衰减
  + 交叉验证
  + 数据集范围的缩小
  + [[https://www.jianshu.com/p/48d391dab189][KNN算法实现及其交叉验证 - 简书]]
  + [[https://www.cnblogs.com/bigmonkey/p/7387943.html][k最邻近算法——加权kNN - 我是8位的 - 博客园]]
  + [[https://www.jianshu.com/p/0feba11bcf82][十折交叉验证 - 简书]]
  + [[https://www.cnblogs.com/itdyb/p/5735911.html][Python合并两个numpy矩阵 - 波比12 - 博客园]]

* com
  + 标称型数据
  + 信息论

* 决策树
  信息熵 -> 条件熵 -> 互信息 -> 相关性

  ID3 等算法缺陷

  协程。

  + [[https://www.cnblogs.com/pinard/p/6050306.html][决策树算法原理(上) - 刘建平Pinard - 博客园]]
  + [[https://www.cnblogs.com/pinard/p/6053344.html][决策树算法原理(下) - 刘建平Pinard - 博客园]]
  + [[https://blog.csdn.net/zjsghww/article/details/51638126][C4.5算法详解（非常仔细） - 张张的专栏 - CSDN博客]]

* 技巧
  + 集成学习，通过多个算法分类，按结果中多的算数
  + log 的使用，避免溢出，还可以将乘法变为加法 log(AB) = log(A) + log(B)

* 贝叶斯
  + [[https://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html][算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification) - T2噬菌体 - 博客园]]

* 理解
  机器学习的过程就是在寻找一个函数，通过这个函数和输入得到响应的输出。

  传统机器学习的过程都是通过多元线性方程来模拟这个函数。

  而深度学习却可以通过跟复杂的方式来模拟更多的函数。

* 数据处理
  标准化 -> 降维 -> 训练

* 参数和超参
  + 参数：就是模型可以根据数据可以自动学习出的变量，应该就是参数。比如，深度学习的权重，偏差等
  + 超参数：就是用来确定模型的一些参数，超参数不同，模型是不同的。
    超参数一般就是根据经验确定的变量。在深度学习中，超参数有：学习速率，迭代次数，层数，每层神经元的个数等等。

* 管道
  数据标准化 -> 特征选取 -> PCA -> 分类器

  标准化和归一化：标准化更常用。

